{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertPreTrainedModel,\n",
    "    BertModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# ----------- GloVe loading functions ----------- #\n",
    "def load_glove_embeddings(glove_path):\n",
    "    \"\"\"\n",
    "    Loads GloVe embeddings from a .txt file into a dict: word -> np.array(float32)\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def text_to_avg_glove(text, embeddings_dict, embed_dim=300):\n",
    "    \"\"\"\n",
    "    Splits text on whitespace, looks up each token in embeddings_dict,\n",
    "    and returns the average embedding. If no known tokens, returns a zero vector.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in embeddings_dict:\n",
    "            vectors.append(embeddings_dict[token])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(embed_dim, dtype=np.float32)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe (this might take a while)...\n",
      "Done loading GloVe!\n",
      "Label distribution in entire dataset:\n",
      "Category\n",
      "0    10271\n",
      "1     1007\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ----- Load GloVe (42B 300d, e.g. glove.42B.300d.txt) -----\n",
    "GLOVE_PATH = \"glove.42B.300d.txt\"\n",
    "print(\"Loading GloVe (this might take a while)...\")\n",
    "glove_dict = load_glove_embeddings(GLOVE_PATH)\n",
    "print(\"Done loading GloVe!\")\n",
    "\n",
    "# ----- Load labeled data -----\n",
    "with open(\"QTL_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "df_labeled = pd.DataFrame(data)\n",
    "\n",
    "# Minimal text preprocessing\n",
    "df_labeled[\"text\"] = (\n",
    "    df_labeled[\"Title\"].fillna(\"\").str.lower() + \" \" +\n",
    "    df_labeled[\"Abstract\"].fillna(\"\").str.lower()\n",
    ")\n",
    "df_labeled[\"Category\"] = df_labeled[\"Category\"].astype(int)\n",
    "\n",
    "# For debug: check distribution\n",
    "print(\"Label distribution in entire dataset:\")\n",
    "print(df_labeled[\"Category\"].value_counts())\n",
    "\n",
    "# ----- Load unlabeled test data -----\n",
    "df_test = pd.read_csv(\"test_unlabeled.tsv\", sep=\"\\t\", dtype={\"PMID\": str})\n",
    "df_test[\"text\"] = (\n",
    "    df_test[\"Title\"].fillna(\"\").str.lower() + \" \" +\n",
    "    df_test[\"Abstract\"].fillna(\"\").str.lower()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution: 0    8216\n",
      "1     806\n",
      "Name: count, dtype: int64\n",
      "Dev distribution: 0    2055\n",
      "1     201\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df_labeled[\"text\"].values\n",
    "y = df_labeled[\"Category\"].values\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train distribution:\", pd.Series(y_train).value_counts())\n",
    "print(\"Dev distribution:\", pd.Series(y_dev).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class BertGloveDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, glove_dict=None, tokenizer=None,\n",
    "                 max_length=256, glove_dim=300):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.glove_dim = glove_dim\n",
    "        \n",
    "        # Precompute GloVe embeddings\n",
    "        self.glove_embs = [\n",
    "            text_to_avg_glove(t, glove_dict, embed_dim=glove_dim) for t in self.texts\n",
    "        ]\n",
    "        \n",
    "        # BERT tokenization\n",
    "        self.encodings = self.tokenizer(\n",
    "            list(self.texts),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"glove_emb\": torch.tensor(self.glove_embs[idx], dtype=torch.float)\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch is a list of dicts from __getitem__()\n",
    "    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n",
    "    attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n",
    "    glove_emb = torch.stack([x[\"glove_emb\"] for x in batch])\n",
    "    \n",
    "    labels = None\n",
    "    if \"labels\" in batch[0]:\n",
    "        labels = torch.stack([x[\"labels\"] for x in batch])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"glove_emb\": glove_emb,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertWithGlove(BertPreTrainedModel):\n",
    "#     def __init__(self, config, glove_dim=300):\n",
    "#         super().__init__(config)\n",
    "#         self.bert = BertModel(config)\n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "#         # BERT hidden size is config.hidden_size (for bert-base-uncased, 768)\n",
    "#         combined_dim = config.hidden_size + glove_dim\n",
    "#         self.classifier = nn.Linear(combined_dim, config.num_labels)\n",
    "        \n",
    "#         self.post_init()  # For transformers >=4.20; otherwise use self.init_weights()\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids=None,\n",
    "#         attention_mask=None,\n",
    "#         glove_emb=None,\n",
    "#         labels=None\n",
    "#     ):\n",
    "#         # Standard BERT pass\n",
    "#         outputs = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         # outputs: (last_hidden_state, pooler_output, hidden_states, attentions)\n",
    "#         pooled_output = outputs.pooler_output  # (batch_size, hidden_size)\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "#         # Concat GloVe\n",
    "#         if glove_emb is None:\n",
    "#             # If for some reason it's missing, fallback\n",
    "#             glove_emb = torch.zeros(pooled_output.size(0), 300).to(pooled_output.device)\n",
    "#         combined = torch.cat((pooled_output, glove_emb), dim=1)  # shape (batch_size, 768+300)\n",
    "        \n",
    "#         logits = self.classifier(combined)\n",
    "        \n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits, labels)\n",
    "        \n",
    "#         return {\n",
    "#             \"loss\": loss,\n",
    "#             \"logits\": logits\n",
    "#         }\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertWithGlove(BertPreTrainedModel):\n",
    "    def __init__(self, config, glove_dim=300):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        combined_dim = config.hidden_size + glove_dim  # 768 + 300\n",
    "        self.classifier = nn.Linear(combined_dim, config.num_labels)\n",
    "        \n",
    "        self.post_init()  # For Transformers >=4.20\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        glove_emb=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output  # shape: (batch_size, 768)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        if glove_emb is None:\n",
    "            glove_emb = torch.zeros(pooled_output.size(0), 300).to(pooled_output.device)\n",
    "\n",
    "        combined = torch.cat((pooled_output, glove_emb), dim=1)  # shape: (batch_size, 1068)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Weighted CrossEntropy: class 0 = 1.0, class 1 = 3.0\n",
    "            class_weights = torch.tensor([1.0, 3.0]).to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertWithGlove were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mohd7\\AppData\\Local\\Temp\\ipykernel_19936\\821244801.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "model = BertWithGlove.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    glove_dim=300,   # pass the extra argument\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_glove_results\",\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Basic metric function\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,   # Optional for classification, but recommended\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6768' max='6768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6768/6768 12:29, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.476006</td>\n",
       "      <td>0.911348</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.390700</td>\n",
       "      <td>0.218380</td>\n",
       "      <td>0.929521</td>\n",
       "      <td>0.563253</td>\n",
       "      <td>0.930348</td>\n",
       "      <td>0.701689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.291374</td>\n",
       "      <td>0.906472</td>\n",
       "      <td>0.486034</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.622540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.346541</td>\n",
       "      <td>0.953014</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.708791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.267686</td>\n",
       "      <td>0.956117</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.751244</td>\n",
       "      <td>0.753117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.246005</td>\n",
       "      <td>0.959663</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.766169</td>\n",
       "      <td>0.771930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6768, training_loss=0.31051809117021856, metrics={'train_runtime': 749.5018, 'train_samples_per_second': 72.224, 'train_steps_per_second': 9.03, 'total_flos': 7121413712424960.0, 'train_loss': 0.31051809117021856, 'epoch': 6.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dev Set Results (argmax) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96      2055\n",
      "           1       0.56      0.93      0.70       201\n",
      "\n",
      "    accuracy                           0.93      2256\n",
      "   macro avg       0.78      0.93      0.83      2256\n",
      "weighted avg       0.95      0.93      0.94      2256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_out = trainer.predict(dev_dataset)\n",
    "dev_logits = pred_out.predictions\n",
    "y_pred_dev = np.argmax(dev_logits, axis=1)\n",
    "\n",
    "print(\"=== Dev Set Results (argmax) ===\")\n",
    "print(classification_report(y_dev, y_pred_dev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dev Set Results (threshold=0.22) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      2055\n",
      "           1       0.80      0.78      0.79       201\n",
      "\n",
      "    accuracy                           0.96      2256\n",
      "   macro avg       0.89      0.88      0.88      2256\n",
      "weighted avg       0.96      0.96      0.96      2256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_probs = torch.softmax(torch.tensor(dev_logits), dim=1).numpy()[:, 1]\n",
    "threshold = 0.22  # you can tune\n",
    "y_pred_dev_custom = (dev_probs >= threshold).astype(int)\n",
    "\n",
    "print(f\"=== Dev Set Results (threshold={threshold}) ===\")\n",
    "print(classification_report(y_dev, y_pred_dev_custom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\llm\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to 'bert_glove_test_2.csv'!\n"
     ]
    }
   ],
   "source": [
    "test_dataset = BertGloveDataset(\n",
    "    df_test[\"text\"].values,\n",
    "    labels=[0] * len(df_test),\n",
    "    glove_dict=glove_dict,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_out = trainer.predict(test_dataset)\n",
    "test_logits = test_out.predictions\n",
    "test_probs = torch.softmax(torch.tensor(test_logits), dim=1).numpy()[:, 1]\n",
    "\n",
    "# Use the threshold that works best for you\n",
    "final_threshold = 0.3\n",
    "df_test[\"Label\"] = (test_probs >= final_threshold).astype(int)\n",
    "\n",
    "df_test[[\"PMID\", \"Label\"]].to_csv(\"bert_glove_test_2.csv\", index=False)\n",
    "print(\"Saved predictions to 'bert_glove_test_2.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.20: 162 predicted 1s out of 1097\n",
      "Threshold 0.25: 161 predicted 1s out of 1097\n",
      "Threshold 0.30: 161 predicted 1s out of 1097\n",
      "Threshold 0.35: 161 predicted 1s out of 1097\n"
     ]
    }
   ],
   "source": [
    "for t in [0.2, 0.25, 0.3, 0.35]:\n",
    "    pred_labels = (test_probs >= t).astype(int)\n",
    "    print(f\"Threshold {t:.2f}: {sum(pred_labels)} predicted 1s out of {len(pred_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold on dev = 0.12 with F1 = 0.7905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_f1 = 0\n",
    "best_t = 0.5\n",
    "for t in np.arange(0.05, 0.5, 0.01):\n",
    "    y_pred = (dev_probs >= t).astype(int)\n",
    "    f1 = f1_score(y_dev, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_t = t\n",
    "\n",
    "print(f\"Best threshold on dev = {best_t:.2f} with F1 = {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
